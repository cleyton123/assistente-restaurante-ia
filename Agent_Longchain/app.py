import os
from dotenv import load_dotenv
load_dotenv()
GROQ_API_KEY = os.getenv("GROQ_API_KEY")

# LangChain
from langchain_groq import ChatGroq
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.chat_history import BaseChatMessageHistory
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory

# Assistente de Restaurante
prompt = ChatPromptTemplate.from_messages([
    (
        "system",
        """
VocÃª Ã© um assistente de restaurante.

FunÃ§Ãµes:
- Receber pedidos
- Sugerir pratos
- Explicar ingredientes
- Confirmar escolhas
- Perguntar preferÃªncias

Primeiro vocÃª deve perguntar:
1. Quantas pessoas vÃ£o pedir?
2. Se jÃ¡ conhecem o cardÃ¡pio?
3. Se querem recomendaÃ§Ãµes da casa?
"""
    ),
    MessagesPlaceholder("history"),
    ("human", "{input}")
])

# LLM 
llm = ChatGroq(
    api_key=GROQ_API_KEY,
    model="llama-3.3-70b-versatile"  
)

chain = prompt | llm

store = {}
#obter o histÃ³rico da sessÃ£o
def get_session_history(session_id: str) -> BaseChatMessageHistory:
    if session_id not in store:
        store[session_id] = ChatMessageHistory()
    return store[session_id]

chain_with_history = RunnableWithMessageHistory(
    chain,
    get_session_history,
    input_messages_key="input",
    history_messages_key="history"
)

#interaÃ§Ã£o com o assistente
def iniciar_assistente_restaurante():
    print("ğŸ½ï¸ Bem-vindo ao Assistente de Restaurante! Digite 'sair' para encerrar.\n")

    while True:
        pergunta = input("Cliente: ")

        if pergunta.lower() in ["sair", "exit"]:
            print("ğŸ‘‹ Obrigado pela visita!")
            break

        resposta = chain_with_history.invoke(
            {"input": pergunta},
            config={"configurable": {"session_id": "mesa01"}}
        )

        print("Assistente:", resposta.content, "\n")

# execuÃ§Ã£o do assistente
if __name__ == "__main__":
    iniciar_assistente_restaurante()
